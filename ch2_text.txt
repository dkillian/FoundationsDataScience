2     High-Dimensional Space
2.1     Introduction

    High dimensional data has become very important. However, high dimensional space
is very different from the two and three dimensional spaces we are familiar with. Generate
n points at random in d-dimensions where each coordinate is a zero mean, unit variance
Gaussian. For sufficiently large d, with high probability the distances between all pairs
of points will be essentially the same. Also the volume of the unit ball in d-dimensions,
the set of all points x such that |x| â‰¤ 1, goes to zero as the dimension goes to infinity.
The volume of a high dimensional unit ball is concentrated near its surface and is also
concentrated at its equator. These properties have important consequences which we will
consider.

2.2     The Law of Large Numbers
    If one generates random points in d-dimensional space using a Gaussian to generate
coordinates, the distance between all pairs of points will be essentially the same when d
is large. The reason is that the square of the distance between two points y and z,
                                                d
                                                X
                                              2
                                      |y âˆ’ z| =   (yi âˆ’ zi )2 ,
                                                   i=1

can be viewed as the sum of d independent samples of a random variable x that is the
squared difference of two Gaussians. In particular, we are summing independent samples
xi = (yi âˆ’zi )2 of a random variable x of bounded variance. In such a case, a general bound
known as the Law of Large Numbers states that with high probability, the average of the
samples will be close to the expectation of the random variable. This in turn implies that
with high probability, the sum is close to the sumâ€™s expectation.

    Specifically, the Law of Large Numbers states that
                                                          
                            x1 + x2 + Â· Â· Â· + xn              V ar(x)
                    Prob                         âˆ’ E(x) â‰¥  â‰¤         .                            (2.1)
                                    n                           n2
The larger the variance of the random variable, the greater the probability that the error
will exceed . Thus the variance of x is in the numerator. The number of samples n is in
the denominator since the more values that are averaged, the smaller the probability that
the difference will exceed . Similarly the larger  is, the smaller the probability that the
    This material has been published by Cambridge University Press as Foundations of Data Science by
Avrim Blum, John Hopcroft, and Ravi Kannan. This pre-publication version is free to view and download
for personal use only. Not for re-distribution, re-sale or use in derivative works. Please do not re-post
or mirror, instead link to http://ttic.edu/blum/book.pdf. c Avrim Blum, John Hopcroft, and Ravi
Kannan 2020. https://www.cambridge.org/9781108485067


                                                   12


--- PAGE BREAK ---

difference will exceed  and hence  is in the denominator. Notice that squaring  makes
the fraction a dimensionless quantity.

    We use two inequalities to prove the Law of Large Numbers. The first is Markovâ€™s
inequality that states that the probability that a non-negative random variable exceeds a
is bounded by the expected value of the variable divided by a.

Theorem 2.1 (Markovâ€™s inequality) Let x be a non-negative random variable. Then
for a > 0,
                                          E(x)
                           Prob(x â‰¥ a) â‰¤       .
                                            a
Proof: For a continuous non-negative random variable x with probability density p,

                             Zâˆ               Za               Zâˆ
                   E (x) =        xp(x)dx =        xp(x)dx +        xp(x)dx
                             0                0                a
                             Zâˆ                   Zâˆ
                         â‰¥        xp(x)dx â‰¥ a          p(x)dx = aProb(x â‰¥ a).
                             a                    a


Thus, Prob(x â‰¥ a) â‰¤ E(x)
                     a
                         .

   The same proof works for discrete random variables with sums instead of integrals.


Corollary 2.2 Prob x â‰¥ bE(x) â‰¤ 1b
                            

  Markovâ€™s inequality bounds the tail of a distribution using only information about the
mean. A tighter bound can be obtained by also using the variance of the random variable.

Theorem 2.3 (Chebyshevâ€™s inequality) Let x be a random variable. Then for c > 0,
                                              V ar(x)
                          Prob |x âˆ’ E(x)| â‰¥ c â‰¤          .
                                                     c2
                                                    
Proof: Prob |x âˆ’ E(x)| â‰¥ c = Prob |x âˆ’ E(x)|2 â‰¥ c2 . Note that y = |x âˆ’ E(x)|2 is a
non-negative random variable and E(y) = V ar(x), so Markovâ€™s inequality can be applied
giving:
                                                  E(|x âˆ’ E(x)|2 )   V ar(x)
     Prob(|x âˆ’ E(x)| â‰¥ c) = Prob |x âˆ’ E(x)|2 â‰¥ c2 â‰¤        2
                                                                   =         .
                                                         c              c2




                                                  13


--- PAGE BREAK ---

   The Law of Large Numbers follows from Chebyshevâ€™s inequality together with facts
about independent random variables. Recall that:

                                E(x + y) = E(x) + E(y),
                                V ar(x âˆ’ c) = V ar(x),
                                V ar(cx) = c2 V ar(x).

Also, if x and y are independent, then E(xy) = E(x)E(y). These facts imply that if x
and y are independent then V ar(x + y) = V ar(x) + V ar(y), which is seen as follows:

           V ar(x + y) = E(x + y)2 âˆ’ E 2 (x + y)
                       = E(x2 + 2xy + y 2 ) âˆ’ E 2 (x) + 2E(x)E(y) + E 2 (y)
                                                                              

                       = E(x2 ) âˆ’ E 2 (x) + E(y 2 ) âˆ’ E 2 (y) = V ar(x) + V ar(y),

where we used independence to replace E(2xy) with 2E(x)E(y).

Theorem 2.4 (Law of Large Numbers) Let x1 , x2 , . . . , xn be n independent samples
of a random variable x. Then
                           x + x + Â·Â·Â· + x              V ar(x)
                             1    2         n
                    Prob                      âˆ’ E(x) â‰¥  â‰¤
                                    n                       n2
                        
Proof: E x1 +x2 +Â·Â·Â·+x
                n
                       n
                           = E(x) and thus
     x + x + Â·Â·Â· + x                   x + x + Â·Â·Â· + x      x + x + Â· Â· Â· + x    
       1   2          n                    1   2          n      1   2           n
Prob                    âˆ’E(x) â‰¥  = Prob                    âˆ’E                      â‰¥
             n                                   n                     n
By Chebyshevâ€™s inequality
     x + x + Â·Â·Â· + x                       x + x + Â·Â·Â· + x          x + x + Â· Â· Â· + x    
       1   2           n                         1      2        n        1     2        n
Prob                     âˆ’ E(x) â‰¥  = Prob                         âˆ’E                       â‰¥
              n                                           n                      n
                                             x1 +x2 +Â·Â·Â·+xn
                                       V ar         n
                                     â‰¤
                                                2
                                        1
                                     = 2 2 V ar(x1 + x2 + Â· Â· Â· + xn )
                                       n
                                        1                                            
                                     = 2 2 V ar(x1 ) + V ar(x2 ) + Â· Â· Â· + V ar(xn )
                                       n
                                       V ar(x)
                                     =         .
                                         n2


    The Law of Large Numbers is quite general, applying to any random variable x of
finite variance. Later we will look at tighter concentration bounds for spherical Gaussians
and sums of 0-1 valued random variables.


                                            14


--- PAGE BREAK ---

    One observation worth making about the Law of Large Numbers is that the size of the
universe does not enter into the bound. For instance, if you want to know what fraction
of the population of a country prefers tea to coffee, then the number n of people you need
to sample in order to have at most a Î´ chance that your estimate is off by more than 
depends only on  and Î´ and not on the population of the country.

    As an application of the Law of Large Numbers, let z be a d-dimensional random point
                                                           1
whose coordinates are each selected from a zero mean, 2Ï€     variance Gaussian. We set the
             1
variance to 2Ï€ so the Gaussian probability density equals one at the origin and is bounded
below throughout the unit ball by a constant.1 By the Law of Large Numbers, the square
of the distance of z to the origin will be Î˜(d) with high probability. In particular, there is
vanishingly small probability that such a random point z would lie in the unit ball. This
implies that the integral of the probability density over the unit ball must be vanishingly
small. On the other hand, the probability density in the unit ball is bounded below by a
constant. We thus conclude that the unit ball must have vanishingly small volume.

   Similarly if we draw two points y and z from a d-dimensional Gaussian with unit
variance in each direction, then |y|2 â‰ˆ d and |z|2 â‰ˆ d. Since for all i,

      E(yi âˆ’ zi )2 = E(yi2 ) + E(zi2 ) âˆ’ 2E(yi zi ) = V ar(yi ) + V ar(zi ) âˆ’ 2E(yi )E(zi ) = 2,
            d
|yâˆ’z|2 =        (yi âˆ’zi )2 â‰ˆ 2d. Thus by the Pythagorean theorem, the random d-dimensional
            P
            i=1
y and z must be approximately orthogonal. This implies that if we scale these random
points to be unit length and call y the North Pole, much of the surface area of the unit ball
must lie near the equator. We will formalize these and related arguments in subsequent
sections.

   We now state a general theorem on probability tail bounds for a sum of indepen-
dent random variables. Tail bounds for sums of Bernoulli, squared Gaussian and Power
Law distributed random variables can all be derived from this. The table in Figure 2.1
summarizes some of the results.

Theorem 2.5 (Master Tail Bounds Theorem) Let x = x1 + x2 + Â· Â· Â· + xn , where
x1 , x2 , . . . , xn are mutually
                             âˆš independent    random variables with zero mean and variance at
most Ïƒ . Let 0 â‰¤ a â‰¤ 2nÏƒ . Assume that |E(xsi )| â‰¤ Ïƒ 2 s! for s = 3, 4, . . . , b(a2 /4nÏƒ 2 )c.
           2                      2

Then,
                                                         2      2
                                    Prob (|x| â‰¥ a) â‰¤ 3eâˆ’a /(12nÏƒ ) .

    The proof of Theorem 2.5 is elementary. A slightly more general version, Theorem 12.5,
is given in the appendix. For a brief intuition of the proof, consider applying Markovâ€™s
inequality to the random variable xr where r is a large even number. Since r is even, xr
  1
   If we instead used variance 1, then the density at the origin would be a decreasing function of d,
          1 d/2
namely ( 2Ï€ ) , making this argument more complicated.


                                                 15


--- PAGE BREAK ---

is non-negative, and thus Prob(|x| â‰¥ a) = Prob(xr â‰¥ ar ) â‰¤ E(xr )/ar . If E(xr ) is not
too large, we will get a good bound. To compute E(xr ), write E(x) as E(x1 + . . . + xn )r
and expand the polynomial into a sum of terms. Use the fact that by independence
         r                   r
E(xri i xj j ) = E(xri i )E(xj j ) to get a collection of simpler expectations that can be bounded
using our assumption that |E(xsi )| â‰¤ Ïƒ 2 s!. For the full proof, see the appendix.

                                  Condition                         Tail bound

       Markov                       xâ‰¥0                         Prob(x â‰¥ a) â‰¤ E(x)
                                                                               a


                                                         Prob |x âˆ’ E(x)| â‰¥ a â‰¤ Var (x)
                                                                            
      Chebyshev                     Any x                                       a2


      Chernoff            x = x1 + x 2 + Â· Â· Â· + x n        Prob(|x âˆ’ E(x)| â‰¥ ÎµE(x))
                                                                          2
                         xi âˆˆ [0, 1] i.i.d. Bernoulli;             â‰¤ 3eâˆ’cÎµ E(x)

  Higher Moments           r positive even integer          Prob(|x| â‰¥ a) â‰¤ E(xr )/ar

                              p                                       âˆš                2
      Gaussian            x = x21 + x22 +âˆšÂ· Â· Â· + x2n     Prob(|x âˆ’    n| â‰¥ Î²) â‰¤ 3eâˆ’cÎ²
      Annulus           xi âˆ¼ N (0, 1); Î² â‰¤ n indep.

                                                                                   
     Power Law             x = x1 + x2 + . . . + xn         Prob |x âˆ’ E(x)| â‰¥ ÎµE(x)
 for xi ; order k â‰¥ 4        xi i.i.d ; Îµ â‰¤ 1/k 2               â‰¤ (4/Îµ2 kn)(kâˆ’3)/2

Figure 2.1: Table of Tail Bounds. The Higher Moments bound is obtained by apply-
ing Markov to xr . The Chernoff, Gaussian Annulus, and Power Law bounds follow from
Theorem 2.5 which is proved in the appendix.


2.3    The Geometry of High Dimensions
   An important property of high-dimensional objects is that most of their volume is
near the surface. Consider any object A in Rd . Now shrink A by a small amount  to
produce a new object (1 âˆ’ )A = {(1 âˆ’ )x|x âˆˆ A}. Then the following equality holds:
                       volume (1 âˆ’ )A = (1 âˆ’ )d volume(A).
                                       

To see that this is true, partition A into infinitesimal cubes. Then, (1 âˆ’ Îµ)A is the union
of a set of cubes obtained by shrinking the cubes in A by a factor of 1 âˆ’ Îµ. When we
shrink each of the 2d sides of a d-dimensional cube by a factor f , its volume shrinks by a
factor of f d . Using the fact that 1 âˆ’ x â‰¤ eâˆ’x , for any object A in Rd we have:
                                            
                            volume (1 âˆ’ )A
                                               = (1 âˆ’ )d â‰¤ eâˆ’d .
                                volume(A)

                                               16


--- PAGE BREAK ---

                                                            Annulus of
                                       1                    width d1
                                        1 âˆ’ d1




Figure 2.2: Most of the volume of the d-dimensional ball of radius r is contained in an
annulus of width O(r/d) near the boundary.


Fixing  and letting d â†’ âˆ, the above quantity rapidly approaches zero. This means
that nearly all of the volume of A must be in the portion of A that does not belong to
the region (1 âˆ’ )A.

    Let S denote the unit ball in d dimensions, that is, the set of points within distance
one of the origin. An immediate implication of the above observation is that at least a
1 âˆ’ eâˆ’d fraction of the volume of the unit ball is concentrated in S \ (1 âˆ’ )S, namely
in a small annulus of width  at the boundary. In particular, most of the volume of the
d-dimensional unit ball is contained in an annulus of width O(1/d) near the boundary. If
                                                     r
                                                       
the ball is of radius r, then the annulus width is O d .

2.4     Properties of the Unit Ball
   We now focus more specifically on properties of the unit ball in d-dimensional space.
We just saw that most of its volume is concentrated in a small annulus of width O(1/d)
near the boundary. Next we will show that in the limit as d goes to infinity, the volume of
the ball goes to zero. This result can be proven in several ways. Here we use integration.

2.4.1   Volume of the Unit Ball
To calculate the volume V (d) of the unit ball in Rd , one can integrate in either Cartesian
or polar coordinates. In Cartesian coordinates the volume is given by
                                    âˆš 2            âˆš
                         xZ1 =1 x2 =Z 1âˆ’x1     xd = 1âˆ’x21 âˆ’Â·Â·Â·âˆ’x2dâˆ’1
                                                      Z
               V (d) =                     Â·Â·Â·                       dxd Â· Â· Â· dx2 dx1 .
                       x1 =âˆ’1
                                    âˆš 2            âˆš 2          2
                            x2 =âˆ’   1âˆ’x1    xd =âˆ’     1âˆ’x1 âˆ’Â·Â·Â·âˆ’xdâˆ’1




                                                 17


--- PAGE BREAK ---

Since the limits of the integrals are complicated, it is easier to integrate using polar
coordinates. In polar coordinates, V (d) is given by

                                                        Z Z1
                                         V (d) =                      rdâˆ’1 drdâ„¦.
                                                        S d r=0

Since the variables â„¦ and r do not interact,
                                        Z          Z1                          Z
                                                            dâˆ’1        1                   A(d)
                              V (d) =        dâ„¦         r         dr =              dâ„¦ =
                                                                       d                    d
                                        Sd     r=0                             Sd

where A(d) is the surface area of the d-dimensional unit ball. For instance, for d = 3 the
                                       4
surface area is 4Ï€ and
                     R the volume is 3 Ï€. The question remains, how to determine the
surface area A (d) = dâ„¦ for general d.
                         Sd

   Consider a different integral
                                  Zâˆ Zâˆ            Zâˆ
                                                        eâˆ’(x1 +x2 +Â·Â·Â·xd ) dxd Â· Â· Â· dx2 dx1 .
                                                                  2    2       2
                        I (d) =              Â·Â·Â·
                                  âˆ’âˆ âˆ’âˆ           âˆ’âˆ

Including the exponential allows integration to infinity rather than stopping at the surface
of the sphere. Thus, I(d) can be computed by integrating in both Cartesian and polar
coordinates. Integrating in polar coordinates will relate I(d) to the surface area A(d).
Equating the two results for I(d) allows one to solve for A(d).

   First, calculate I(d) by integration in Cartesian coordinates.
                                        ï£® âˆ     ï£¹d
                                                    âˆš d
                                         Z
                                             2            d
                                I (d) = ï£° eâˆ’x dxï£» =  Ï€ = Ï€2.
                                             âˆ’âˆ
                                   Râˆ     2     âˆš
Here, we have used the fact that âˆ’âˆ eâˆ’x dx = Ï€. For a proof of this, see Section 12.2
of the appendix. Next, calculate I(d) by integrating in polar coordinates. The volume of
the differential element is rdâˆ’1 dâ„¦dr. Thus,
                                                   Z          Zâˆ
                                                                           2
                                        I (d) =          dâ„¦           eâˆ’r rdâˆ’1 dr.
                                                   Sd         0
               R
The integral        dâ„¦ is the integral over the entire solid angle and gives the surface area,
               Sd
                                                                      Râˆ âˆ’r2 dâˆ’1
A(d), of a unit sphere. Thus, I (d) = A (d)                             e r dr. Evaluating the remaining
                                                                      0


                                                             18


--- PAGE BREAK ---

integral gives
               Zâˆ                           Zâˆ                                  1 Zâˆ                   
                          âˆ’r2                             dâˆ’1
                                                                
                                                                      âˆ’ 12
                                                                                          d
                                                                                       âˆ’t 2 âˆ’ 1     1   d
                      e         rdâˆ’1 dr =         eâˆ’t t    2        1
                                                                    2
                                                                      t      dt =     e t       dt = Î“
                                                                                  2                 2   2
                  0                         0                                            0

                                            d
and hence, I(d) = A(d) 12 Î“ 2 where the Gamma function Î“ (x) is a generalization of the
                                              

               âˆš for non-integer values of x. Î“ (x) = (x âˆ’ 1) Î“ (x âˆ’ 1), Î“ (1) = Î“ (2) = 1,
factorial function
and Î“ 21 = Ï€. For integer x, Î“ (x) = (x âˆ’ 1)!.
                                      d
    Combining I (d) = Ï€ 2 with I (d) = A (d) 12 Î“                                d
                                                                                     
                                                                                 2
                                                                                         yields
                                                                                 d
                                                                          Ï€2
                                                                A (d) = 1 d 
                                                                        2
                                                                          Î“ 2
establishing the following lemma.
Lemma 2.6 The surface area A(d) and the volume V (d) of a unit-radius ball in d di-
mensions are given by
                                                            d                                     d
                                              2Ï€ 2                                       2Ï€ 2
                                       A (d) = d                          and   V (d) =           .
                                              Î“( 2 )                                    d Î“( d2 )

    To check the formula for the volume of a unit ball, note that V (2) = Ï€ and V (3) =
     3
2 Ï€2
3 Î“( 3 )
           = 34 Ï€, which are the correct volumes for the unit balls in two and three dimen-
     2
sions. To check the formula for the surface area of a unit ball, note that A(2) = 2Ï€ and
              3
A(3) = 2Ï€
        1âˆš
          2
           Ï€
             = 4Ï€, which are the correct surface areas for the unit ball in two and three
        2
                         d
dimensions. Note that Ï€ 2 is an exponential in d2 and Î“ d2 grows as the factorial of d2 .
                                                            

This implies that lim V (d) = 0, as claimed.
                                dâ†’âˆ



2.4.2       Volume Near the Equator
    An interesting fact about the unit ball in high dimensions is that most of its volume
is concentrated near its â€œequatorâ€. In particular, for any unit-length vector v defining
â€œnorthâ€, most of the volume of theâˆšunit ball lies in the thin slab of points whose dot-
product with v has magnitude O(1/ d). To show this fact, it suffices by symmetry to fix
v to be the first coordinate
                          âˆš vector. That is, we will show that most of the volume of the
unit ball has |x1 | = O(1/ d). Using this fact, we will show that two random points in the
unit ball are with high probability nearly orthogonal, and also give an alternative proof
from the one in Section 2.4.1 that the volume of the unit ball goes to zero as d â†’ âˆ.
                                                                                             2
Theorem 2.7 For c â‰¥ 1 and d â‰¥ 3, at least a 1 âˆ’ 2c eâˆ’c /2 fraction of the volume of the
                                      c
d-dimensional unit ball has |x1 | â‰¤ âˆšdâˆ’1 .

                                                                           19


--- PAGE BREAK ---

                                                                      2
Proof: By symmetry we just need to prove that at most a 2c eâˆ’c /2 fraction of the half of
                                c                                                     c
the ball with x1 â‰¥ 0 has x1 â‰¥ âˆšdâˆ’1 . Let A denote the portion of the ball with x1 â‰¥ âˆšdâˆ’1
and let H denote the upper hemisphere. We will then show that the ratio of the volume
of A to the volume of H goes to zero by calculating an upper bound on volume(A) and
a lower bound on volume(H) and proving that
                       volume(A)   upper bound volume(A)  2 c2
                                 â‰¤                       = eâˆ’ 2 .
                       volume(H)   lower bound volume(H)  c


   To calculate the volume of A, integrate an incremental volumep      that is a disk of width
dx1 and whose face is a ball of dimension d âˆ’ 1 and radius 1 âˆ’ x21 . The surface area of
                      dâˆ’1
the disk is (1 âˆ’ x21 ) 2 V (d âˆ’ 1) and the volume above the slice is
                                        Z 1
                                                       dâˆ’1
                          volume(A) =        (1 âˆ’ x21 ) 2 V (d âˆ’ 1)dx1
                                         âˆšc
                                          dâˆ’1

                                                                 âˆ’x
To get an upper bound  âˆš on the above integral, use 1 âˆ’ x â‰¤ e       and integrate to infinity.
To integrate, insert x1 cdâˆ’1 , which is greater than one in the range of integration, into the
integral. Then
               Z âˆ      âˆš                                         âˆš
                     x1 d âˆ’ 1 âˆ’ dâˆ’1 x21                             dâˆ’1 âˆ
                                                                          Z
                                                                                     dâˆ’1 2
 volume(A) â‰¤                     e  2    V (d âˆ’ 1)dx1 = V (d âˆ’ 1)               x1 eâˆ’ 2 x1 dx1
                âˆšc        c                                          c      âˆšc
                 dâˆ’1                                                          dâˆ’1


Now             Z âˆ
                             dâˆ’1 2             1 âˆ’ dâˆ’1 x21 âˆ          1 âˆ’ c2
                         x1 eâˆ’ 2 x1 dx1 = âˆ’       e 2             =       e 2
                  âˆšc                          dâˆ’1          âˆš c
                                                            (dâˆ’1)
                                                                    d âˆ’ 1
                   dâˆ’1

                                                      c2
Thus, an upper bound on volume(A) is Vcâˆš(dâˆ’1)
                                          dâˆ’1
                                              eâˆ’ 2 .
                                                        1
 The volume of the hemisphere below the plane x1 = âˆšdâˆ’1     is a lower bound on the entire
                                                                                         1
volume of the upper hemisphere and this volume is at least that of a cylinder of height âˆšdâˆ’1
           q                                                             dâˆ’1
                  1                                                  1        1
and radius 1 âˆ’ dâˆ’1   . The volume of the cylinder is V (d âˆ’ 1)(1 âˆ’ dâˆ’1  ) 2 âˆšdâˆ’1 . Using the
fact that (1âˆ’x)a â‰¥ 1âˆ’ax for a â‰¥ 1, the volume of the cylinder is at least V2âˆš(dâˆ’1)
                                                                               dâˆ’1
                                                                                   for d â‰¥ 3.

   Thus,
                                                                      2
                                                            V (dâˆ’1) âˆ’ c
                           upper bound above plane           âˆš
                                                            c dâˆ’1
                                                                    e 2    2 c2
              ratio â‰¤                                 =        V (dâˆ’1)
                                                                          = eâˆ’ 2
                         lower bound total hemisphere           âˆš          c
                                                               2 dâˆ’1



One might ask why we computed a lower bound on the total hemisphere since it is one
half of the volume of the unit ball which we already know. The reason is that the volume
of the upper hemisphere is 12 V (d) and we need a formula with V (d âˆ’ 1) in it to cancel the
V (d âˆ’ 1) in the numerator.

                                                 20


--- PAGE BREAK ---

                                                 x1
                                        ï£±
                                        ï£²
                                    H        A
                                        ï£³                 âˆšc
                                                           dâˆ’1




Figure 2.3: Most of the volume of the upper hemisphere of the d-dimensional ball is
                       c
below the plane x1 = âˆšdâˆ’1 .

Near orthogonality. One immediate implication of the above analysis is that if we
draw two points at random from the unit ball, with high probability their vectors will be
nearly orthogonal to each other. Specifically, from our previous analysis in Section 2.3,
with high probability both will be close to the surface and will have length 1 âˆ’ O(1/d).
From our analysis above, if we define the vector in the direction of the first pointâˆš as
â€œnorthâ€, with high probability the second will have a âˆš   projection of only Â±O(1/ d) in
this direction, and thus their dot-product will be Â±O(1/ d). This âˆš  implies that with high
probability, the angle between the two vectors will be Ï€/2 Â± O(1/ d). In particular, we
have the following theorem that states that if we draw n points at random in the unit
ball, with high probability all points will be close to unit length and each pair of points
will be almost orthogonal.
Theorem 2.8 Consider drawing n points x1 , x2 , . . . , xn at random from the unit ball.
With probability 1 âˆ’ O(1/n)
  1. |xi | â‰¥ 1 âˆ’ 2 lnd n for all i, and
                  âˆš
  2. |xi Â· xj | â‰¤ âˆš6dâˆ’1
                     ln n
                          for all i 6= j.
Proof: For the first part, for any fixed i by the analysis of Section 2.3, the probability
that |xi | < 1 âˆ’  is less than eâˆ’d . Thus
                                            2 ln n       2 ln n
                          Prob |xi | < 1 âˆ’           â‰¤ eâˆ’( d )d = 1/n2 .
                                               d
By the union bound, the probability there exists an i such that |xi | < 1 âˆ’ 2 lnd n is at most
1/n.

    For the second part, Theorem 2.7 states that for a component of a Gaussian vector
                                                c2
                          c
                                is at most 2c eâˆ’ 2 . There are n2 pairs i and j and for each such
                                                                 
the probability |xi | > âˆšdâˆ’1
pair if we define xi as â€œnorthâ€,
                           âˆš
                                     the probability that the projection of xj onto the â€œnorthâ€
                                                        6 ln n
direction is more than âˆšdâˆ’1 is at most O(eâˆ’ 2 ) = O(nâˆ’3 ). Thus, the dot-product
                             6 ln n

condition is violated with probability at most O n2 nâˆ’3 = O(1/n) as well.
                                                                


                                                 21


--- PAGE BREAK ---

                                                                   âˆš
            1                      1                                 d
                âˆš                                                   2
                  2                        1
                 2                                         1

      1                      1                        1
      2                      2                        2
                                                                 â† Unit radius sphere

                                                                 â†âˆ’ Nearly all the volume
                                                                   â† Vertex of hypercube


Figure 2.4: Illustration of the relationship between the sphere and the cube in 2, 4, and
d-dimensions.


Alternative proof that volume goes to zero. Another immediate implication of
Theorem 2.7 is that as d â†’ âˆ, the volume of the ball approaches zero. Specifically, con-
                                                         2c
sider a small box centered at the origin of side length âˆšdâˆ’1 . Using Theorem 2.7, we show
               âˆš
that for c = 2 ln d, this box contains over half of the volume of the ball. On the other
hand, the volume of this box clearly goes to zero as d goes to infinity, since its volume is
    ln d d/2
O(( dâˆ’1 ) ). Thus the volume of the ball goes to zero as well.
                              âˆš                                                           c
    By Theorem 2.7 with c = 2 ln d, the fraction of the volume of the ball with |x1 | â‰¥ âˆšdâˆ’1
is at most:
                         2 âˆ’ c2     1 âˆ’2 ln d         1       1
                           e 2 =âˆš        e     = âˆš         < 2.
                         c          ln d          d2 ln d    d
Since this is true for each of the d dimensions, by a union bound at most a O( d1 ) â‰¤ 12
fraction of the volume of the ball lies outside the cube, completing the proof.

Discussion. One might wonder how it can be that nearly all the points in the unit ball
are very close to the    surface and yet at the same time nearly all points are in a box of
                   ln d
side-length O dâˆ’1 . The answer is to remember that points on the surface of the ball
                                                                                         
satisfy x1 + x2 + . . . + xd = 1, so for each coordinate i, a typical value will be Â±O âˆš1d .
          2     2           2

In fact, it is often helpful to think of picking
                                                a random point on the sphere as very similar
                                               1     1    1         1
to picking a random point of the form Â± âˆšd , Â± âˆšd , Â± âˆšd , . . . Â± âˆšd .

2.5       Generating Points Uniformly at Random from a Ball

    Consider generating points uniformly at random on the surface of the unit ball. For
the 2-dimensional version of generating points on the circumference of a unit-radius cir-
cle, independently generate each coordinate uniformly at random from the interval [âˆ’1, 1].
This produces points distributed over a square that is large enough to completely contain
the unit circle. Project each point onto the unit circle. The distribution is not uniform

                                               22


--- PAGE BREAK ---

since more points fall on a line from the origin to a vertex of the square than fall on a line
from the origin to the midpoint of an edge of the square due to the difference in length.
To solve this problem, discard all points outside the unit circle and project the remaining
points onto the circle.

     In higher dimensions, this method does not work since the fraction of points that fall
inside the ball drops to zero and all of the points would be thrown away. The solution is to
generate a point each of whose coordinates is an independent Gaussian variable. Generate
x1 , x2 , . . . , xd , using a zero mean, unit variance Gaussian, namely, âˆš12Ï€ exp(âˆ’x2 /2) on the
real line.2 Thus, the probability density of x is

                                                 1           x21 +x22 +Â·Â·Â·+x2d
                                                         âˆ’
                                  p (x) =            d e
                                                                      2
                                              (2Ï€) 2

and is spherically symmetric. Normalizing the vector x = (x1 , x2 , . . . , xd ) to a unit vector,
         x
namely |x| , gives a distribution that is uniform over the surface of the sphere. Note that
once the vector is normalized, its coordinates are no longer statistically independent.

    To generate a point y uniformly over the ball (surface and interior), scale the point
 x
|x|
    generated on the surface by a scalar Ï âˆˆ [0, 1].     What should the distribution of Ï be
as a function of r? It is certainly not uniform, even in 2 dimensions. Indeed, the density
of Ï at r is proportional to r for d = 2. For d = 3, it is proportional to r2 . By similar
                                                                   dâˆ’1
reasoning,
R r=1 dâˆ’1 the density of Ï at distance r is proportional to r          in d dimensions. Solving
 r=0
     cr dr = 1 (the integral of density must equal 1) one should set c = d. Another
way to see this formally is that the volume of the radius r ball in d dimensions is rd V (d).
                                     d
The density at radius r is exactly dr  (rd Vd ) = drdâˆ’1 Vd . So, pick Ï(r) with density equal to
drdâˆ’1 for r over [0, 1].

       We have succeeded in generating a point
                                                        x
                                              y=Ï
                                                       |x|

uniformly at random from the unit ball by using the convenient spherical Gaussian dis-
tribution. In the next sections, we will analyze the spherical Gaussian in more detail.
   2
    One might naturally ask: â€œhow do you generate a random number from a 1-dimensional Gaussian?â€
To generate a number from any distribution given its cumulative distribution function P, first select a
uniform random number u âˆˆ [0, 1] and then choose x = P âˆ’1 (u). For any a < b, the probability that x is
between a and b is equal to the probability that u is between P (a) and P (b) which equals P (b) âˆ’ P (a)
as desired. For the 2-dimensional Gaussian,
                                          p one can generate a point in polar coordinates by choosing
angle Î¸ uniform in [0, 2Ï€] and radius r = âˆ’2 ln(u) where u is uniform random in [0, 1]. This is called
the Box-Muller transform.




                                                  23


--- PAGE BREAK ---

2.6    Gaussians in High Dimension
     A 1-dimensional Gaussian has its mass close to the origin. However, as the dimension
is increased something different happens. The d-dimensional spherical Gaussian with zero
mean and variance Ïƒ 2 in each coordinate has density function
                                         1           
                                                         |x|2
                                                              
                              p(x) =             exp   âˆ’ 2Ïƒ 2
                                                                .
                                     (2Ï€)d/2 Ïƒ d

The value of the density is maximum at the origin, but there is very little volume there.
When Ïƒ 2 = 1, integrating the probability density over a unit ball centered at the origin
yields almost zero mass since the volume of âˆš  such a ball is negligible. In fact, one needs
to increase the radius of the ball to nearly d before there is a significant volume âˆš and
hence significant probability mass. If one increases the radius much beyond d, the
integral barely increases even though the volume increases since the probability density
is dropping off at a much higher rate. The following theorem formally statesâˆš    that nearly
all the probability is concentrated in a thin annulus of width O(1) at radius d.

Theorem 2.9 (Gaussian Annulus Theorem) âˆš         For a d-dimensional spherical Gaussian
                                                                          2
                                      for any Î² â‰¤ d,âˆšall but at most 3eâˆ’cÎ² of the prob-
with unit variance in each direction, âˆš
ability mass lies within the annulus d âˆ’ Î² â‰¤ |x| â‰¤ d + Î², where c is a fixed positive
constant.
                                                      d
For a high-level intuition, note that E(|x|2 ) =            E(x2i ) = dE(x21 ) = d, so the mean
                                                      P
                                                      i=1
squared distance of a point from the center is d. The Gaussian Annulus Theorem says
that the points are
                 âˆš tightly concentrated. We call the square root of the mean squared
distance, namely d, the radius of the Gaussian.

   To prove the Gaussian Annulus Theorem we make use of a tail inequality for sums of
independent random variables of bounded moments (Theorem 12.5).

Proof (Gaussian Annulus Theorem): Let x = (x1 , x2 , . . . , xd ) be âˆš a point selected
from a unit variance Gaussianâˆšcentered at the âˆš
âˆš                                              origin, and let r = |x|. d âˆ’ Î² â‰¤ |x| â‰¤
  d+âˆš  Î² is equivalent to |r âˆ’ d|
                               âˆš  â‰¥ Î². If
                                       âˆš  |r âˆ’  d| â‰¥ Î², then multiplying both sides by
                2
r + d givesâˆš|r âˆ’ d| â‰¥ Î²(r + d) â‰¥ Î² d. So, it suffices to bound the probability that
|r2 âˆ’ d| â‰¥ Î² d.

    Rewrite r2 âˆ’ d = (x21 + . . . + x2d ) âˆ’ d = (x21 âˆ’ 1) + . . . + (x2d âˆ’ 1) and perform a change
                                                                                               âˆš
of variables: yi = x2i âˆ’ 1. We want to bound the probability that |y1 + . . . + yd | â‰¥ Î² d.
Notice that E(yi ) = E(x2i ) âˆ’ 1 = 0. To apply Theorem 12.5, we need to bound the sth
moments of yi .




                                               24


--- PAGE BREAK ---

   For |xi | â‰¤ 1, |yi |s â‰¤ 1 and for |xi | â‰¥ 1, |yi |s â‰¤ |xi |2s . Thus
                        |E(yis )| = E(|yi |s ) â‰¤ E(1 + x2s            2s
                                                         i ) = 1 + E(xi )
                                        r Z âˆ
                                             2            2
                                  =1+              x2s eâˆ’x /2 dx
                                            Ï€ 0
Using the substitution 2z = x2 ,
                                                     Z âˆ
                                              1
                             |E(yis )| = 1 + âˆš             2s z sâˆ’(1/2) eâˆ’z dz
                                                 Ï€    0
                                      â‰¤ 2s s!.
The last inequality is from the Gamma integral.

    Since E(yi ) = 0, V ar(yi ) = E(yi2 ) â‰¤ 22 2 = 8. Unfortunately, we do not have |E(yis )| â‰¤
8s! as required in Theorem 12.5. To fix this problem, perform one more change of variables,
using wi = yi /2. Then, V ar(wi ) â‰¤ 2âˆšand |E(wis )| â‰¤ 2s!, and our goal is now to bound the
probability that |w1 + . . . + wd | â‰¥ Î² 2 d . Applying Theorem 12.5 where Ïƒ 2 = 2 and n = d,
                                                                 Î²2
this occurs with probability less than or equal to 3eâˆ’ 96 .

   In the next sections we will see several uses of the Gaussian Annulus Theorem.

2.7    Random Projection and Johnson-Lindenstrauss Lemma
    One of the most frequently used subroutines in tasks involving high dimensional data
is nearest neighbor search. In nearest neighbor search we are given a database of n points
in Rd where n and d are usually large. The database can be preprocessed and stored in
an efficient data structure. Thereafter, we are presented â€œqueryâ€ points in Rd and are
asked to find the nearest or approximately nearest database point to the query point.
Since the number of queries is often large, the time to answer each query should be very
small, ideally a small function of log n and log d, whereas preprocessing time could be
larger, namely a polynomial function of n and d. For this and other problems, dimension
reduction, where one projects the database points to a k-dimensional space with k  d
(usually dependent on log d) can be very useful so long as the relative distances between
points are approximately preserved. We will see using the Gaussian Annulus Theorem
that such a projection indeed exists and is simple.

   The projection f : Rd â†’ Rk that we will examine (many related projections are
known to work as well) is the following. Pick k Gaussian vectors u1 , u2 , . . . , uk in Rd
with unit-variance coordinates. For any vector v, define the projection f (v) by:
                               f (v) = (u1 Â· v, u2 Â· v, . . . , uk Â· v).
The projection f (v) is the vectorâˆšof dot products of v with the ui . We will show that
with high probability, |f (v)| â‰ˆ k|v|. For any two vectors v1 and v2 , f (v1 âˆ’ v2 ) =

                                                     25


--- PAGE BREAK ---

f (v1 ) âˆ’ f (v2 ). Thus, to estimate the distance |v1 âˆ’ v2 | between two vectors v1 and v2 in
Rd , it sufficesâˆšto compute |f (v1 ) âˆ’ f (v2 )| = |f (v1 âˆ’ v2 )| in the k-dimensional space since
the factor of k is known and one can divide by it. The reason distances increase when
we project to a lower dimensional space is that the vectors ui are not unit length. Also
notice that the vectors ui are not orthogonal. If we had required them to be orthogonal,
we would have lost statistical independence.

Theorem 2.10 (The Random Projection Theorem) Let v be a fixed vector in Rd
and let f be defined as above. There exists constant c > 0 such that for Îµ âˆˆ (0, 1),
                                     âˆš           âˆš               2
                     Prob |f (v)| âˆ’ k|v| â‰¥ Îµ k|v| â‰¤ 3eâˆ’ckÎµ ,

where the probability is taken over the random draws of vectors ui used to construct f .

Proof: By scaling both sides of the inner inequality by |v|, we may assume that |v| = 1.
The sum of independent normally distributed real variables is also normally distributed
where thePmean    and variance are the sums of the individual means and variances. Since
           d
ui Â· v = j=1 uij vj , the random variable ui Â· v has Gaussian density with zero mean and
unit variance, in particular,
                                    d
                                             !    d                  d
                                   X             X                   X
                                                      2
              V ar(ui Â· v) = V ar      uij vj =      vj V ar(uij ) =   vj2 = 1
                                      j=1            j=1                j=1


Since u1 Â·v, u2 Â·v, . . . , uk Â·v are independent Gaussian random variables, f (v) is a random
vector from a k-dimensional spherical Gaussian with unit variance in each coordinate, and
so the theorem follows from the Gaussian Annulus Theorem (Theorem 2.9) with d replaced
by k.

   The random projection theorem establishes that the probability of the length of the
projection of a single vector differing significantly from its expected value is exponentially
small in k, the dimension of the target subspace. By a union bound, the probability that
any of O(n2 ) pairwise differences |vi âˆ’ vj | among n vectors v1 , . . . , vn differs significantly
from their expected values is small, provided k â‰¥ cÎµ32 ln n. Thus, this random projection
preserves all relative pairwise distances between points in a set of n points with high
probability. This is the content of the Johnson-Lindenstrauss Lemma.

Theorem 2.11 (Johnson-Lindenstrauss Lemma) For any 0 < Îµ < 1 and any integer
n, let k â‰¥ cÎµ32 ln n with c as in Theorem 2.9. For any set of n points in Rd , the random
projection f : Rd â†’ Rk defined above has the property that for all pairs of points vi and
vj , with probability at least 1 âˆ’ 3/2n,
                        âˆš                                            âˆš
                 (1 âˆ’ Îµ) k |vi âˆ’ vj | â‰¤ |f (vi ) âˆ’ f (vj )| â‰¤ (1 + Îµ) k |vi âˆ’ vj | .



                                                26


--- PAGE BREAK ---

Proof: Applying the Random Projection Theorem (Theorem 2.10), for any fixed vi and
vj , the probability that |f (vi âˆ’ vj )| is outside the range
                           h       âˆš                     âˆš          i
                            (1 âˆ’ Îµ) k|vi âˆ’ vj |, (1 + Îµ) k|vi âˆ’ vj |
                 2
is at most 3eâˆ’ckÎµ â‰¤ 3/n3 for k â‰¥ 3cÎµln2n . Since there are n2 < n2 /2 pairs of points, by the
                                                             
                                                                                  3
union bound, the probability that any pair has a large distortion is less than 2n   .

Remark: It is important to note that the conclusion of Theorem 2.11 asserts for all vi
and vj , not just for most of them. The weaker assertion for most vi and vj is typically less
useful, since our algorithm for a problem such as nearest-neighbor search might return
one of the bad pairs of points. A remarkable aspect of the theorem is that the number
of dimensions in the projection is only dependent logarithmically on n. Since k is often
much less than d, this is called a dimension reduction technique. In applications, the
dominant term is typically the 1/Îµ2 term.

    For the nearest neighbor problem, if the database has n1 points and n2 queries are
expected during the lifetime of the algorithm, take n = n1 + n2 and project the database
to a random k-dimensional space, for k as in Theorem 2.11. On receiving a query, project
the query to the same subspace and compute nearby database points. The Johnson
Lindenstrauss Lemma says that with high probability this will yield the right answer
whatever the query. Note that the exponentially small in k probability was useful here in
making k only dependent on ln n, rather than n.

2.8    Separating Gaussians
Mixtures of Gaussians are often used to model heterogeneous data coming from multiple
sources. For example, suppose we are recording the heights of individuals age 20-30 in a
city. We know that on average, men tend to be taller than women, so a natural model
would be a Gaussian mixture model p(x) = w1 p1 (x) + w2 p2 (x), where p1 (x) is a Gaussian
density representing the typical heights of women, p2 (x) is a Gaussian density represent-
ing the typical heights of men, and w1 and w2 are the mixture weights representing the
proportion of women and men in the city. The parameter estimation problem for a mixture
model is the problem: given access to samples from the overall density p (e.g., heights of
people in the city, but without being told whether the person with that height is male
or female), reconstruct the parameters for the distribution (e.g., good approximations to
the means and variances of p1 and p2 , as well as the mixture weights).

    There are taller women and shorter men, so even if one solved the parameter estima-
tion problem for heights perfectly, given a data point, one couldnâ€™t necessarily tell which
population it came from. That is, given a height, one couldnâ€™t necessarily tell if it came
from a man or a woman. In this section, we will look at a problem that is in some ways
easier and some ways harder than this problem of heights. It will be harder in that we
will be interested in a mixture of two Gaussians in high-dimensions as opposed to the

                                             27


--- PAGE BREAK ---

                                                                âˆ†                      âˆš
                                               x                                  z      2d
                         âˆš                                 âˆš                             y
           âˆš              2d              âˆš                    âˆ†2 + 2d
               d                           d
                     âˆš
                         d                     p                âˆ†                 q



               (a)                                              (b)


Figure 2.5: (a) indicates that two randomly chosen points in high dimension are surely
almost nearly orthogonal. (b) indicates the distance between a pair of random points
from two different unit balls approximating the annuli of two Gaussians.


d = 1 case of heights. But it will be easier in that we will assume the means are quite
well-separated compared to the variances. Specifically, our focus will be on a mixture of
two spherical unit-variance Gaussians whose means are separated by a distance â„¦(d1/4 ).
We will show that at this level of separation, we can with high probability uniquely de-
termine which Gaussian each data point came from. The algorithm to do so will actually
be quite simple. Calculate the distance between all pairs of points. Points whose distance
apart is smaller are from the same Gaussian, whereas points whose distance is larger are
from different Gaussians. Later, we will see that with more sophisticated algorithms, even
a separation of â„¦(1) suffices.

    First, consider just one spherical unit-variance Gaussian centered at the origin. From    âˆš
Theorem 2.9, most     of   its probability mass   lies on an annulus of width O(1)  at radius   d.
       âˆ’|x|2 /2
                 Q âˆ’x2 /2
Also e          = ie     i     and almost all of the mass is within the slab { x | âˆ’c â‰¤ x1 â‰¤ c },
for c âˆˆ O(1). Pick a point x from this Gaussian. After picking x, rotate the coordinate
system to make the first axis align with x. Independently pick a second point y from
this Gaussian. The fact that almost all of the probability mass of the Gaussian is within
the slab {x | âˆ’ c â‰¤ x1 â‰¤ c, c âˆˆ O(1)} at the equator implies that yâ€™s component along
xâ€™s directionp is O(1) with high probability. Thus, y is nearly perpendicular to x. So,
|x âˆ’ y| â‰ˆ |x|2 + |y|2 . See Figure 2.5(a). More precisely,      âˆš since the coordinate system
has been rotated so that x is at the North Pole, x = ( d Â± O(1), 0, . . . , 0). Since y is
almost on the equator, further rotate the coordinate system so that the component of
y that is perpendicular
              âˆš               to the axis of the North Pole is in the second coordinate. Then
y = (O(1), d Â± O(1), 0, . . . , 0). Thus,
                                           âˆš               âˆš              âˆš
                     (x âˆ’ y)2 = d Â± O( d) + d Â± O( d) = 2d Â± O( d)
                 âˆš
and |x âˆ’ y| = 2d Â± O(1) with high probability.



                                                   28


--- PAGE BREAK ---

    Consider two spherical unit variance Gaussians with centers p and q separated by a
distance âˆ†. The distance between a randomly chosen point   âˆš x from the first Gaussian
and a randomly chosen point y from the second is close to âˆ†2 + 2d, since x âˆ’ p, p âˆ’ q,
and q âˆ’ y are nearly mutually perpendicular. Pick x and rotate the coordinate system
so that x is at the North Pole. Let z be the North Pole of the ball approximating the
second Gaussian. Now pick y. Most of the mass of the second Gaussian is within O(1)
of the equator perpendicular to z âˆ’ q. Also, most of the mass of each Gaussian is within
distance O(1) of the respective equators perpendicular to the line q âˆ’ p. See Figure 2.5
(b). Thus,

                               |x âˆ’ y|2 â‰ˆ âˆ†2 + |z âˆ’ q|2 + |q âˆ’ y|2
                                                        âˆš
                                        = âˆ†2 + 2d Â± O( d).

    To ensure that two points picked from the same Gaussian are closer to each other
than two points picked from different Gaussians requires that the upper limit of the
distance between a pair of points from the same Gaussian is at most the   âˆš lower limit
of
âˆš distance between pointsâˆšfrom different  Gaussians. This requires that 2d + O(1) â‰¤
   2d + âˆ†2 âˆ’O(1) or 2d+O( d) â‰¤ 2d+âˆ†2 , which holds when âˆ† âˆˆ Ï‰(d1/4 ). Thus, mixtures
of spherical Gaussians can be separated in this way, provided their centers are separated
by Ï‰(d1/4 ). If we have n points and want to correctly separate all of them with high
probability, we need our individual high-probability statements to hold withâˆšprobability
1 âˆ’ 1/poly(n),3 which means ourâˆš O(1) terms from Theorem 2.9 become O( log n). So
we need to include an extra O( log n) term in the separation distance.

        Algorithm for separating points from two Gaussians: Calculate all
        pairwise distances between points. The cluster of smallest pairwise distances
        must come from a single Gaussian. Remove these points. The remaining
        points come from the second Gaussian.

One can actually separate Gaussians where the centers are much closer. In the next
chapter we will use singular value decomposition to separate points from a mixture of two
Gaussians when their centers are separated by a distance O(1).

2.9       Fitting a Spherical Gaussian to Data
   Given a set of sample points, x1 , x2 , . . . , xn , in a d-dimensional space, we wish to find
the spherical Gaussian that best fits the points. Let f be the unknown Gaussian with
mean Âµ and variance Ïƒ 2 in each direction. The probability density for picking these points
when sampling according to f is given by
                                                                             !
                             (x1 âˆ’ Âµ)2 + (x2 âˆ’ Âµ)2 + Â· Â· Â· + (xn âˆ’ Âµ)2
                  c exp âˆ’
                                                       2Ïƒ 2
  3
      poly(n) means bounded by a polynomial in n.


                                                    29


--- PAGE BREAK ---

                                                                 n
                                                                 Z
                                                                          |xâˆ’Âµ|2
                                                                      âˆ’
where the normalizing constant c is the reciprocal of e      dx . In integrating from
                                                                            2Ïƒ 2

                                                        Z          âˆ’n
                                                             |x|2
                                                            âˆ’ 2
âˆ’âˆ to âˆ, one can shift the origin to Âµ and thus c is       e 2Ïƒ dx      = 1 n2 and is
                                                                                     (2Ï€)
independent of Âµ.

   The Maximum Likelihood Estimator (MLE) of f, given the samples x1 , x2 , . . . , xn , is
the f that maximizes the above probability density.

Lemma 2.12 Let {x1 , x2 , . . . , xn } be a set of n d-dimensional points. Then (x1 âˆ’ Âµ)2 +
(x2 âˆ’ Âµ)2 +Â· Â· Â·+(xn âˆ’ Âµ)2 is minimized when Âµ is the centroid of the points x1 , x2 , . . . , xn ,
namely Âµ = n1 (x1 + x2 + Â· Â· Â· + xn ).

Proof: Setting the gradient of (x1 âˆ’ Âµ)2 + (x2 âˆ’ Âµ)2 + Â· Â· Â· + (xn âˆ’ Âµ)2 with respect to Âµ
to zero yields
                   âˆ’2 (x1 âˆ’ Âµ) âˆ’ 2 (x2 âˆ’ Âµ) âˆ’ Â· Â· Â· âˆ’ 2 (xn âˆ’ Âµ) = 0.
Solving for Âµ gives Âµ = n1 (x1 + x2 + Â· Â· Â· + xn ).

   To determine the maximum likelihood estimate of Ïƒ 2 for f , set Âµ to the true centroid.
Next, show that Ïƒ is set to the standard deviation of the sample. Substitute Î½ = 2Ïƒ1 2 and
a = (x1 âˆ’ Âµ)2 + (x2 âˆ’ Âµ)2 + Â· Â· Â· + (xn âˆ’ Âµ)2 into the formula for the probability of picking
the points x1 , x2 , . . . , xn . This gives

                                               eâˆ’aÎ½
                                                         n .
                                          R
                                              eâˆ’x2 Î½ dx
                                          x

Now, a is fixed and Î½ is to be determined. Taking logs, the expression to maximize is
                                           ï£®          ï£¹
                                            Z
                                                   2
                                âˆ’aÎ½ âˆ’ n ln ï£° eâˆ’Î½x dxï£» .
                                                     x

To find the maximum, differentiate with respect to Î½, set the derivative to zero, and solve
for Ïƒ. The derivative is                 R 2 âˆ’Î½x2
                                           |x| e   dx
                                         x
                                 âˆ’a + n R âˆ’Î½x2         .
                                             e    dx
                                                 x
             âˆš
Setting y = | Î½x| in the derivative, yields
                                                            2
                                                y 2 eâˆ’y dy
                                                 R
                                           n y
                                      âˆ’a +     R           .
                                           Î½     eâˆ’y2 dy
                                                     y


                                                 30


--- PAGE BREAK ---

Since the ratio of the two integrals is the expected distance squared of a d-dimensional
spherical Gaussian of standard deviation âˆš12 to its center, and this is known to be d2 , we
get âˆ’a + nd
          2Î½
                                     1
             . Substituting Ïƒ 2 for 2Î½
                                     âˆš
                                         gives âˆ’a + ndÏƒ 2 . Setting âˆ’a + ndÏƒ 2 = 0 shows that
                                       a
the maximum occurs when Ïƒ = âˆšnd           . Note that this quantity is the square root of the
average coordinate distance squared of the samples to their mean, which is the standard
deviation of the sample. Thus, we get the following lemma.

Lemma 2.13 The maximum likelihood spherical Gaussian for a set of samples is the
Gaussian with center equal to the sample mean and standard deviation equal to the stan-
dard deviation of the sample from the true mean.

    Let x1 , x2 , . . . , xn be a sample of points generated by a Gaussian probability distri-
bution. Then Âµ = n1 (x1 + x2 + Â· Â· Â· + xn ) is an unbiased estimator of the expected value
of the distribution. However, if in estimating the variance from the sample set, we use
the estimate of the expected value rather than the true expected value, we will not get
an unbiased estimate of the variance, since the sample mean is not independent of the
                                         1
sample set. One should use ÂµÌƒ = nâˆ’1         (x1 + x2 + Â· Â· Â· + xn ) when estimating the variance.
See Section 12.4.10 of the appendix.

2.10     Bibliographic Notes
   The word vector model was introduced by Salton [SWY75]. There is vast literature on
the Gaussian distribution, its properties, drawing samples according to it, etc. The reader
can choose the level and depth according to his/her background. The Master Tail Bounds
theorem and the derivation of Chernoff and other inequalities from it are from [Kan09].
The original proof of the Random Projection Theorem by Johnson and Lindenstrauss was
complicated. Several authors used Gaussians to simplify the proof. The proof here is due
to Dasgupta and Gupta [DG99]. See [Vem04] for details and applications of the theorem.
[MU05] and [MR95b] are text books covering much of the material touched upon here.




                                               31


--- PAGE BREAK ---

2.11     Exercises
Exercise 2.1
  1. Let x and y be independent random variables with uniform distribution in [0, 1].
     What is the expected value E(x), E(x2 ), E(x âˆ’ y), E(xy), and E(x âˆ’ y)2 ?

  2. Let x and y be independent random variables with uniform distribution in [âˆ’ 21 , 12 ].
     What is the expected value E(x), E(x2 ), E(x âˆ’ y), E(xy), and E(x âˆ’ y)2 ?

  3. What is the expected squared distance between two points generated at random inside
     a unit d-dimensional cube?

Exercise 2.2 Randomly generate 30 points inside the cube [âˆ’ 21 , 12 ]100 and plot distance
between points and the angle between the vectors from the origin to the points for all pairs
of points.

Exercise 2.3 Show that for any a â‰¥ 1 there exist distributions for which Markovâ€™s in-
equality is tight by showing the following:
  1. For each a = 2, 3, and 4 give a probability
                                        E(x) distribution p(x) for a non-negative
     random variable x where Prob x â‰¥ a = a .

  2. For arbitrary a â‰¥ 1 give
                           aE(x)
                              probability distribution for a non-negative random variable
     x where Prob x â‰¥ a = a .

Exercise 2.4 Show that for any c â‰¥ 1 there exist distributions for which Chebyshevâ€™s
inequality is tight, in other words, P rob(|x âˆ’ E(x)| â‰¥ c) = V ar(x)/c2 .

Exercise 2.5 Let x be a random variable with probability density 41 for 0 â‰¤ x â‰¤ 4 and
zero elsewhere.
  1. Use Markovâ€™s inequality to bound the probability that x â‰¥ 3.

  2. Make use of Prob(|x| â‰¥ a) = Prob(x2 â‰¥ a2 ) to get a tighter bound.

  3. What is the bound using Prob(|x| â‰¥ a) = Prob(xr â‰¥ ar )?

Exercise 2.6 Consider the probability distribution p(x = 0) = 1 âˆ’ a1 and p(x = a) = a1 .
Plot the probability that x is greater than or equal to a as a function of a for the bound
given by Markovâ€™s inequality and by Markovâ€™s inequality applied to x2 and x4 .

Exercise 2.7 Consider the probability density function p(x) = 0 for x < 1 and p(x) = c x14
for x â‰¥ 1.
  1. What should c be to make p a legal probability density function?

  2. Generate 100 random samples from this distribution. How close is the average of
     the samples to the expected value of x?

                                            32


--- PAGE BREAK ---

Exercise 2.8 Let U be a set of integers and X and Y be subsets of U whose symmetric
difference Xâˆ†Y is 1/10 of U. Prove that the probability that none of the elements selected
at random from U will be in Xâˆ†Y is less than eâˆ’0.1n .

Exercise 2.9 Let G be a d-dimensional spherical Gaussian with variance 21 in each di-
rection, centered at the origin. Derive the expected squared distance to the origin.

Exercise 2.10 Consider drawing a random point x on the surface of the unit sphere in
Rd . What is the variance of x1 (the first coordinate of x)? See if you can give an argument
without doing any integrals.

Exercise 2.11 How large must Îµ be for 99% of the volume of a 1000-dimensional unit-
radius ball to lie in the shell of Îµ-thickness at the surface of the ball?

Exercise 2.12 Prove that 1 + x â‰¤ ex for all real x. For what values of x is the approxi-
mation 1 + x â‰ˆ ex within 0.01?

Exercise 2.13 For what value of d does the volume, V (d), of a d-dimensional unit ball
take on its maximum? Hint: Consider the ratio V V(dâˆ’1)
                                                  (d)
                                                       .

Exercise 2.14 A 3-dimensional cube has vertices, edges, and faces. In a d-dimensional
cube, these components are called faces. A vertex is a 0-dimensional face, an edge a
1-dimensional face, etc.

  1. For 0 â‰¤ k â‰¤ d, how many k-dimensional faces does a d-dimensional cube have?

  2. What is the total number of faces of all dimensions? The d-dimensional face is the
     cube itself which you can include in your count.

  3. What is the surface area of a unit cube in d-dimensions (a unit cube has side-length
     equal to 1 in each dimension)?

  4. What is the surface area of the cube if the length of each side was 2?

  5. Prove that the volume of a unit cube is close to its surface.

Exercise 2.15 Consider the portion of the surface area of a unit radius, 3-dimensional
ball with center at the origin that lies within a circular cone whose vertex is at the origin.
What is the formula for the incremental unit of area when using polar coordinates to
integrate the portion of the surface area of the ball that is lying inside the circular cone?
What is the formula for the integral? What is the value of the integral if the angle of the
cone is 36â—¦ ? The angle of the cone is measured from the axis of the cone to a ray on the
surface of the cone.




                                             33


--- PAGE BREAK ---

Exercise 2.16 Consider a unit radius, circular cylinder in 3-dimensions of height one.
The top of the cylinder could be an horizontal plane or half of a circular ball. Consider
these two possibilities for a unit radius, circular cylinder in 4-dimensions. In 4-dimensions
the horizontal plane is 3-dimensional and the half circular ball is 4-dimensional. In each
of the two cases, what is the surface area of the top face of the cylinder? You can use
V (d) for the volume of a unit radius, d-dimension ball and A(d) for the surface area of
a unit radius, d-dimensional ball. An infinite length, unit radius, circular cylinder in 4-
dimensions would be the set {(x1 , x2 , x3 , x4 )|x22 + x23 + x24 â‰¤ 1} where the coordinate x1 is
the axis.

Exercise 2.17 Given a d-dimensional circular cylinder of radius r and height h

  1. What is the surface area in terms of V (d) and A(d)?

  2. What is the volume?

Exercise 2.18 How does the volume of a ball of radius two behave as the dimension of
the space increases? What if the radius was larger than two but a constant independent
of d? What function of d would the radius need to be for a ball of radius r to have
approximately constant volume as the dimension increases? Hint: you may want to use
                                 n
Stirlingâ€™s approximation, n! â‰ˆ ne , for factorial.

Exercise 2.19 If lim V (d) = 0, the volume of a d-dimensional ball for sufficiently large
                    dâ†’âˆ
d must be less than V (3). How can this be if the d-dimensional ball contains the three
dimensional ball?

Exercise 2.20

  1. Write a recurrence relation for V (d) in terms of V (d âˆ’ 1) by integrating over x1 .
     Hint: At x1 = t, the (d âˆ’ 1)-dimensional
                                         âˆš       volume of the slice is the volume of a
     (d âˆ’ 1)-dimensional sphere of radius 1 âˆ’ t2 . Express this in terms of V (d âˆ’ 1) and
     write down the integral. You need not evaluate the integral.

  2. Verify the formula for d = 2 and d = 3 by integrating and comparing with V (2) = Ï€
     and V (3) = 43 Ï€

Exercise 2.21 Consider a unit ball A centered at the origin and a unit ball B whose
center is at distance s from the origin. Suppose that a random point x is drawn from
the mixture distribution: â€œwith probability 1/2, draw at randomâˆš from A; with probability
1/2, draw at random from Bâ€. Show that a separation s  1/ d âˆ’ 1 is sufficientâˆš    so that
Prob(x âˆˆ A âˆ© B) = o(1); i.e., for any  > 0 there exists c such that if s â‰¥ c/ d âˆ’ 1, then
Prob(x âˆˆ A âˆ© B) < . In other words, this extent of separation means that nearly all of
the mixture distribution is identifiable.



                                               34


--- PAGE BREAK ---

Exercise 2.22 Consider the upper hemisphere of a unit-radius ball in d-dimensions.
What is the height of the maximum volume cylinder that can be placed entirely inside
the hemisphere? As you increase the height of the cylinder, you need to reduce the cylin-
derâ€™s radius so that it will lie entirely within the hemisphere.

Exercise 2.23 What is the volume of the maximum size d-dimensional hypercube that
can be placed entirely inside a unit radius d-dimensional ball?

Exercise 2.24 Calculate the ratio of area above the plane x1 =  to the area of the upper
hemisphere of a unit radius ball in d-dimensions for  = 0.001, 0.01, 0.02, 0.03, 0.04, 0.05
and for d = 100 and d = 1, 000.

Exercise 2.25 Almost all of the volume of a ball in high dimensions lies in a narrow
slice of the ball at the equator. However, the narrow slice is determined by the point on
the surface of the ball that is designated the North Pole. Explain how this can be true
if several different locations are selected for the location of the North Pole giving rise to
different equators.

Exercise 2.26 Explain how the volume of a ball in high dimensions can simultaneously
be in a narrow slice at the equator and also be concentrated in a narrow annulus at the
surface of the ball.

Exercise 2.27 Generate 500 points uniformly at random on the surface of a unit-radius
ball in 50 dimensions. Then randomly generate five additional points. For each of the five
new points, calculate a narrow band of width âˆš250 at the equator, assuming the point was
the North Pole. How many of the 500 points are in each band corresponding to one of the
five equators? How many of the points are in all five bands? How wide do the bands need
to be for all points to be in all five bands?

Exercise 2.28 Place 100 points at random on a d-dimensional unit-radius ball. Assume
d is large. Pick a random vector and let it define two parallel hyperplanes on opposite
sides of the origin that are equal distance from the origin. How close can the hyperplanes
be moved and still have at least a .99 probability that all of the 100 points land between
them?

Exercise 2.29 Let x and y be d-dimensional zero mean, unit variance Gaussian vectors.
Prove that x and y are almost orthogonal by considering their dot product.

Exercise 2.30 Prove that with high probability, the angle between two random vectors in
a high-dimensional space is at least 45â—¦ . Hint: use Theorem 2.8.
                                                                        âˆš
Exercise 2.31 Project the volume of a d-dimensional ball of radius d onto a line
through the center. For large d, give an intuitive argument that the projected volume
should behave like a Gaussian.


                                             35


--- PAGE BREAK ---

Exercise 2.32

  1. Write a computer program that generates n points uniformly distributed over the
     surface of a unit-radius d-dimensional ball.

  2. Generate 200 points on the surface of a sphere in 50 dimensions.

  3. Create several random lines through the origin and project the points onto each line.
     Plot the distribution of points on each line.

  4. What does your result from (3) say about the surface area of the sphere in relation
     to the lines, i.e., where is the surface area concentrated relative to each line?

Exercise 2.33 If one generates points in d-dimensions with each coordinate a unit vari-
                                                                                     âˆš
ance Gaussian, the points will approximately lie on the surface of a sphere of radius d.

  1. What is the distribution when the points are projected onto a random line through
     the origin?

  2. If one uses a Gaussian with variance four, where in d-space will the points lie?

Exercise 2.34 Randomly generate a 100 points on the surface of a sphere in 3-dimensions
and in 100-dimensions. Create a histogram of all distances between the pairs of points in
both cases.

Exercise 2.35 We have claimed that a randomly generated point on a ball lies near the
equator of the ball, independent of the point picked to be the North Pole. Is the same claim
true for a randomly generated point on a cube? To test this claim, randomly generate ten
Â±1 valued vectors in 128 dimensions. Think of these ten vectors as ten choices for the
North Pole. Then generate some additional Â±1 valued vectors. To how many of the
original vectors is each of the new vectors close to being perpendicular; that is, how many
of the equators is each new vector close to?

Exercise
 d      2.36 Define the equator of a d-dimensional unit cube to be the hyperplane
      xi = d2 .
   P
 x
    i=1

  1. Are the vertices of a unit cube concentrated close to the equator?

  2. Is the volume of a unit cube concentrated close to the equator?

  3. Is the surface area of a unit cube concentrated close to the equator?

Exercise 2.37 Consider a non-orthogonal basis e1 , e2 , . . . , ed . The ei are a set of linearly
independent unit vectors that span the space.

  1. Prove that the representation of any vector in this basis is unique.

                                               36


--- PAGE BREAK ---

                                             âˆš
  2. Calculate the squared
                         âˆš âˆš
                             length of z = ( 22 , 1)e where z is expressed in the basis e1 =
     (1, 0) and e2 = (âˆ’ 22 , 22 )
              P                   P
  3. If y = i ai ei and z = i bi ei , with 0 < ai < bi , is it necessarily true that the
     length of z is greater than the length of y? Why or why not?
                                                   âˆš    âˆš
  4. Consider the basis e1 = (1, 0) and e2 = (âˆ’ 22 , 22 ).

       (a) What is the representation of the vector (0,1) in the basis (e1 , e2 ).
                                                        âˆš   âˆš
       (b) What is the representation of the vector ( 22 , 22 )?
       (c) What is the representation of the vector (1, 2)?




  e2                           e2                               e2


                      e1                           e1                            e1




Exercise 2.38 Generate 20 points uniformly at random on a 900-dimensional sphere of
radius 30. Calculate the distance between each pair of points. Then, select a method of
projection and project the data ontoâˆšsubspaces of dimension k=100, 50, 10, 5, 4, 3, 2, 1
and calculate the difference between k times the original distances and the newâˆšpair-wise
distances. For each value of k what is the maximum difference as a percent of k.

Exercise 2.39 What happens in high dimension to a lower dimensional manifold? To
see what happens, consider a sphere of dimension 100 in a 1,000 dimensional space when
the 1,000 dimensional space is projected to a random 500 dimensional space. Will the
sphere remain essentially spherical? Given an intuitive argument justifying your answer.

Exercise 2.40 In d-dimensions there are exactly d-unit vectors that are pairwise orthog-
onal. However, if you wanted a set of vectors that were almost orthogonal you might
squeeze in a few more. For example, in 2-dimensions if almost orthogonal meant at least
45 degrees apart, you could fit in three almost orthogonal vectors. Suppose you wanted to
find 1000 almost orthogonal vectors in 100 dimensions. Here are two ways you could do
it:
  1. Begin with 1,000 orthonormal 1,000-dimensional vectors, and then project them to
     a random 100-dimensional space.

  2. Generate 1000 100-dimensional random Gaussian vectors.

                                             37


--- PAGE BREAK ---

Implement both ideas and compare them to see which does a better job.

Exercise 2.41 Suppose there is an object moving at constant velocity along a straight
line. You receive the gps coordinates corrupted by Gaussian noise every minute. How do
you estimate the current position?

Exercise 2.42
  1. What is the maximum size rectangle that can be fitted under a unit variance Gaus-
     sian?

  2. What unit area rectangle best approximates a unit variance Gaussian if one measure
     goodness of fit by the symmetric difference of the Gaussian and the rectangle.

Exercise 2.43 Let x1 , x2 , . . . , xn be independent samples of a random variable x with
                                            n
mean Âµ and variance Ïƒ 2 . Let ms = n1
                                           P
                                              xi be the sample mean. Suppose one estimates
                                       i=1
the variance using the sample mean rather than the true mean, that is,
                                            n
                                        1   X
                                  Ïƒs2 =         (xi âˆ’ ms )2
                                        n i=1

Prove that E(Ïƒs2 ) = nâˆ’1
                      n
                         Ïƒ 2 and thus one should have divided by n âˆ’ 1 rather than n.

Hint: First calculate the variance
                              Pn of the sample   mean and show that var(ms ) = n1 var(x).
                    2       1             2
Then calculate E(Ïƒs ) = E[ n i=1 (xi âˆ’ms ) ] by replacing xi âˆ’ms with (xi âˆ’m)âˆ’(ms âˆ’m).

Exercise 2.44 Generate ten values by a Gaussian probability distribution with zero mean
and variance one. What is the center determined by averaging the points? What is the
variance? In estimating the variance, use both the real center and the estimated center.
When using the estimated center to estimate the variance, use both n = 10 and n = 9.
How do the three estimates compare?

Exercise 2.45 Suppose you want to estimate the unknown center of a Gaussian in d-
space which has variance one in each direction. Show that O(log d/Îµ2 ) random samples
from the Gaussian are sufficient to get an estimate ms of the true center Âµ, so that with
probability at least 99%,
                                   kÂµ âˆ’ ms kâˆ â‰¤ Îµ.
How many samples are sufficient to ensure that with probability at least 99%

                                    kÂµ âˆ’ ms k2 â‰¤ Îµ?
                                                                 2
                                                          1 (xâˆ’5)
Exercise 2.46 Use the probability distribution 3âˆš12Ï€ eâˆ’ 2     9      to generate ten points.

(a) From the ten points estimate Âµ. How close is the estimate of Âµ to the true mean of
     5?

                                             38

